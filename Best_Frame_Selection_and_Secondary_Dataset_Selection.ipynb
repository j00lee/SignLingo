{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j00lee/SignLingo/blob/main/Best_Frame_Selection_and_Secondary_Dataset_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing on small sample of dataset"
      ],
      "metadata": {
        "id": "ERr-dYSRujaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==2.0.0 --force-reinstall\n",
        "!pip install mediapipe --no-deps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUkNV5nN0MdG",
        "outputId": "aad3d8fb-f621-475c-f74d-39cf72f241a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==2.0.0\n",
            "  Using cached numpy-2.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Using cached numpy-2.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.0\n",
            "    Uninstalling numpy-2.0.0:\n",
            "      Successfully uninstalled numpy-2.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mediapipe 0.10.21 requires sounddevice>=0.4.4, which is not installed.\n",
            "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.0.0 which is incompatible.\n",
            "mediapipe 0.10.21 requires protobuf<5,>=4.25.3, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.0\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AdLiw44S0eCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0673b8f1-b8f8-4a30-c0a9-3144b4a15274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "ZPD6ggr8ubL1",
        "outputId": "f9d0b096-11b9-48d5-c1a6-618ea61752b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Processing train split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train videos:   0%|          | 5/21240 [00:01<1:55:06,  3.07it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-7ffe71b17a0b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0msharpness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_sharpness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mhas_hands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_hands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_hands\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msharpness\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score_with_hands\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-7ffe71b17a0b>\u001b[0m in \u001b[0;36mdetect_hands\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetect_hands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mimage_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhands_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_rgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_hand_landmarks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mediapipe/python/solutions/hands.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \"\"\"\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mediapipe/python/solution_base.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    338\u001b[0m                                      data).at(self._simulated_timestamp))\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0;31m# Create a NamedTuple object where the field names are mapping to the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;31m# output stream names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mediapipe as mp\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Set up MediaPipe Hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands_detector = mp_hands.Hands(static_image_mode=True, max_num_hands=2)\n",
        "\n",
        "def calculate_sharpness(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "\n",
        "def detect_hands(image):\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = hands_detector.process(image_rgb)\n",
        "    return results.multi_hand_landmarks is not None\n",
        "\n",
        "# === Paths\n",
        "base_dataset_path = '/content/drive/MyDrive/ASL Project/dataset'\n",
        "base_filtered_splits = '/content/drive/MyDrive/ASL Project/filtered splits'\n",
        "base_best_frames = '/content/drive/MyDrive/ASL Project/best_frames'\n",
        "\n",
        "splits = ['train', 'val', 'test']\n",
        "# sample_size = 10  # Number of videos to sample per split\n",
        "\n",
        "# === Hyperparameters\n",
        "sharpness_threshold = 70.0  # Change this to be more/less strict\n",
        "\n",
        "# === Logs\n",
        "missing_videos = []\n",
        "dirty_videos = []\n",
        "\n",
        "for split in splits:\n",
        "    print(f\"🚀 Processing {split} split...\")\n",
        "\n",
        "    frames_path = os.path.join(base_dataset_path, split)\n",
        "    output_path = os.path.join(base_best_frames, split)\n",
        "    dirty_output_path = os.path.join(base_best_frames, f'dirty_{split}')\n",
        "\n",
        "    # === Clear and recreate folders\n",
        "    if os.path.exists(output_path):\n",
        "        shutil.rmtree(output_path)\n",
        "    os.makedirs(output_path)\n",
        "\n",
        "    if os.path.exists(dirty_output_path):\n",
        "        shutil.rmtree(dirty_output_path)\n",
        "    os.makedirs(dirty_output_path)\n",
        "\n",
        "    # Load CSV and sample videos\n",
        "    csv_path = os.path.join(base_filtered_splits, f'{split}_filtered.csv')\n",
        "    df = pd.read_csv(csv_path)  # Comma-separated\n",
        "    video_list = [os.path.splitext(v)[0] for v in df['Video file'].tolist()]\n",
        "    # sample_videos = random.sample(video_list, min(sample_size, len(video_list)))\n",
        "    sample_videos = video_list\n",
        "\n",
        "    sharpness_log = []\n",
        "\n",
        "    for video_folder in tqdm(sample_videos, desc=f\"Processing {split} videos\"):\n",
        "        frame_folder = os.path.join(frames_path, video_folder)\n",
        "\n",
        "        if not os.path.isdir(frame_folder):\n",
        "            print(f\"⚠️ Warning: Frame folder missing for {video_folder}\")\n",
        "            missing_videos.append(video_folder)\n",
        "            continue\n",
        "\n",
        "        best_frame_with_hands = None\n",
        "        best_score_with_hands = -1\n",
        "\n",
        "        for frame_file in sorted(os.listdir(frame_folder)):\n",
        "            frame_path = os.path.join(frame_folder, frame_file)\n",
        "            frame = cv2.imread(frame_path)\n",
        "            if frame is None:\n",
        "                continue\n",
        "\n",
        "            sharpness = calculate_sharpness(frame)\n",
        "            has_hands = detect_hands(frame)\n",
        "\n",
        "            if has_hands and sharpness > best_score_with_hands:\n",
        "                best_score_with_hands = sharpness\n",
        "                best_frame_with_hands = frame\n",
        "\n",
        "        if best_frame_with_hands is not None:\n",
        "            if best_score_with_hands >= sharpness_threshold:\n",
        "                save_path = os.path.join(output_path, f'{video_folder}.jpg')\n",
        "                cv2.imwrite(save_path, best_frame_with_hands)\n",
        "                sharpness_log.append((video_folder, best_score_with_hands))\n",
        "            else:\n",
        "                print(f\"⚠️ Frame for {video_folder} below threshold ({best_score_with_hands:.2f})\")\n",
        "                dirty_save_path = os.path.join(dirty_output_path, f'{video_folder}.jpg')\n",
        "                cv2.imwrite(dirty_save_path, best_frame_with_hands)\n",
        "                dirty_videos.append(video_folder)\n",
        "        else:\n",
        "            print(f\"⚠️ No frame with hands found for {video_folder}\")\n",
        "            dirty_videos.append(video_folder)\n",
        "\n",
        "    # Save sharpness log for this split\n",
        "    sharpness_df = pd.DataFrame(sharpness_log, columns=[\"video\", \"sharpness\"])\n",
        "    sharpness_df = sharpness_df.sort_values(by=\"sharpness\", ascending=False)\n",
        "    sharpness_df.to_csv(os.path.join(base_best_frames, f\"{split}_sharpness.csv\"), index=False)\n",
        "\n",
        "# === Save final logs\n",
        "missing_log_path = os.path.join(base_best_frames, 'missing_videos.txt')\n",
        "dirty_log_path = os.path.join(base_best_frames, 'dirty_videos.txt')\n",
        "\n",
        "with open(missing_log_path, 'w') as f:\n",
        "    for video in missing_videos:\n",
        "        f.write(f\"{video}\\n\")\n",
        "\n",
        "with open(dirty_log_path, 'w') as f:\n",
        "    for video in dirty_videos:\n",
        "        f.write(f\"{video}\\n\")\n",
        "\n",
        "print(\"✅ Done selecting best frames for sampled videos!\")\n",
        "print(f\"⚡ Missing videos logged to: {missing_log_path}\")\n",
        "print(f\"⚡ Dirty videos logged to: {dirty_log_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mediapipe as mp\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# --- CONFIG ---\n",
        "BASE_DATASET_PATH = '/content/drive/MyDrive/ASL Project/dataset'\n",
        "BASE_SPLITS_PATH = '/content/drive/MyDrive/ASL Project/filtered splits'\n",
        "BASE_OUTPUT_PATH = '/content/drive/MyDrive/ASL Project/best_frames'\n",
        "SHARPNESS_THRESHOLD = 70.0\n",
        "SPLITS = ['train', 'val', 'test']\n",
        "NUM_WORKERS = 2  # Limit parallel workers to reduce memory use\n",
        "CHUNK_SIZE = 1000  # Process in smaller chunks to avoid memory overload\n",
        "\n",
        "# --- SHARED PROCESSING FUNCTION ---\n",
        "def process_video(args):\n",
        "    video_folder, split, gloss_lookup = args\n",
        "\n",
        "    frames_path = os.path.join(BASE_DATASET_PATH, split, video_folder)\n",
        "    output_path = os.path.join(BASE_OUTPUT_PATH, split, f\"{video_folder}.jpg\")\n",
        "    dirty_path = os.path.join(BASE_OUTPUT_PATH, f'dirty_{split}', f\"{video_folder}.jpg\")\n",
        "\n",
        "    gloss = gloss_lookup.get(video_folder, \"\")\n",
        "\n",
        "    if os.path.exists(output_path) or os.path.exists(dirty_path):\n",
        "        return ('skipped', video_folder, gloss, None, None)\n",
        "\n",
        "    if not os.path.isdir(frames_path):\n",
        "        return ('missing', video_folder, gloss, None, None)\n",
        "\n",
        "    try:\n",
        "        mp_hands = mp.solutions.hands.Hands(static_image_mode=True, max_num_hands=2)\n",
        "        best_frame = None\n",
        "        best_score = -1\n",
        "\n",
        "        for frame_file in sorted(os.listdir(frames_path)):\n",
        "            frame_path = os.path.join(frames_path, frame_file)\n",
        "            try:\n",
        "                frame = cv2.imread(frame_path)\n",
        "                if frame is None:\n",
        "                    continue\n",
        "\n",
        "                # Optional: downsample to save memory\n",
        "                # frame = cv2.resize(frame, (640, 480))\n",
        "\n",
        "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "                sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                results = mp_hands.process(rgb)\n",
        "\n",
        "                if results.multi_hand_landmarks and sharpness > best_score:\n",
        "                    best_score = sharpness\n",
        "                    best_frame = frame\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing frame {frame_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        mp_hands.close()\n",
        "\n",
        "        if best_frame is None:\n",
        "            return ('dirty', video_folder, gloss, None, None)\n",
        "\n",
        "        if best_score >= SHARPNESS_THRESHOLD:\n",
        "            return ('clean', video_folder, gloss, best_score, best_frame)\n",
        "        else:\n",
        "            return ('dirty', video_folder, gloss, best_score, best_frame)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing video {video_folder}: {e}\")\n",
        "        return ('dirty', video_folder, gloss, None, None)\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "def process_split(split):\n",
        "    print(f\"\\n🚀 Processing split: {split}\")\n",
        "\n",
        "    output_dir = os.path.join(BASE_OUTPUT_PATH, split)\n",
        "    dirty_dir = os.path.join(BASE_OUTPUT_PATH, f'dirty_{split}')\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(dirty_dir, exist_ok=True)\n",
        "\n",
        "    csv_path = os.path.join(BASE_SPLITS_PATH, f'{split}_filtered.csv')\n",
        "    df = pd.read_csv(csv_path)\n",
        "    video_list = [os.path.splitext(v)[0] for v in df['Video file'].tolist()]\n",
        "    gloss_lookup = {os.path.splitext(row['Video file'])[0]: row['Gloss'] for _, row in df.iterrows()}\n",
        "    args = [(v, split, gloss_lookup) for v in video_list]\n",
        "\n",
        "    print(f\"📦 {len(args)} videos to process with {NUM_WORKERS} workers in chunks of {CHUNK_SIZE}\")\n",
        "\n",
        "    clean_entries = []\n",
        "    dirty_entries = []\n",
        "    missing_videos = []\n",
        "    skipped_videos = []\n",
        "    sharpness_log_path = os.path.join(BASE_OUTPUT_PATH, f'{split}_sharpness.csv')\n",
        "\n",
        "    if os.path.exists(sharpness_log_path):\n",
        "        existing_sharpness_df = pd.read_csv(sharpness_log_path)\n",
        "        processed_videos = set(existing_sharpness_df['video'].tolist())\n",
        "    else:\n",
        "        processed_videos = set()\n",
        "\n",
        "    for i in range(0, len(args), CHUNK_SIZE):\n",
        "        chunk = args[i:i + CHUNK_SIZE]\n",
        "        with Pool(NUM_WORKERS) as pool:\n",
        "            for result in tqdm(pool.imap_unordered(process_video, chunk), total=len(chunk)):\n",
        "                if result[0] == 'skipped':\n",
        "                    skipped_videos.append(result[1])\n",
        "                    continue\n",
        "                elif result[0] == 'missing':\n",
        "                    missing_videos.append(result[1])\n",
        "                    continue\n",
        "\n",
        "                status, video_name, gloss, sharpness, frame = result\n",
        "                if status == 'dirty':\n",
        "                    if sharpness is not None:\n",
        "                        dirty_entries.append((video_name, gloss, sharpness))\n",
        "                    if frame is not None:\n",
        "                        cv2.imwrite(os.path.join(dirty_dir, f'{video_name}.jpg'), frame)\n",
        "                elif status == 'clean':\n",
        "                    clean_entries.append((video_name, gloss, sharpness))\n",
        "                    cv2.imwrite(os.path.join(output_dir, f'{video_name}.jpg'), frame)\n",
        "\n",
        "    all_entries = clean_entries + dirty_entries\n",
        "    all_df = pd.DataFrame(all_entries, columns=[\"video\", \"gloss\", \"sharpness\"])\n",
        "    if os.path.exists(sharpness_log_path):\n",
        "        prev_df = pd.read_csv(sharpness_log_path)\n",
        "        all_df = pd.concat([prev_df, all_df], ignore_index=True)\n",
        "\n",
        "    all_df = all_df.drop_duplicates(subset=\"video\").sort_values(by=\"sharpness\", ascending=False)\n",
        "    all_df.to_csv(sharpness_log_path, index=False)\n",
        "\n",
        "    with open(os.path.join(BASE_OUTPUT_PATH, 'missing_videos.txt'), 'a') as f:\n",
        "        for v in missing_videos:\n",
        "            f.write(f\"{v}\\n\")\n",
        "\n",
        "    with open(os.path.join(BASE_OUTPUT_PATH, 'dirty_videos.txt'), 'a') as f:\n",
        "        for v in [entry[0] for entry in dirty_entries if entry[0]]:\n",
        "            f.write(f\"{v}\\n\")\n",
        "\n",
        "    print(f\"✅ {split} done: {len(clean_entries)} clean, {len(dirty_entries)} dirty, {len(missing_videos)} missing, {len(skipped_videos)} skipped\")\n",
        "\n",
        "# --- RUN ALL SPLITS ---\n",
        "if __name__ == '__main__':\n",
        "    for split in SPLITS:\n",
        "        process_split(split)\n",
        "\n",
        "    print(\"\\n🎉 All splits completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "6x_IMrQ6FOzF",
        "outputId": "cac4a83f-f103-4a03-e2bf-eb647fbbd502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Processing split: train\n",
            "📦 21240 videos to process with 2 workers in chunks of 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 998/1000 [39:00<00:04,  2.34s/it]Process ForkPoolWorker-16:\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-7c656028fb0b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSPLITS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mprocess_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n🎉 All splits completed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-7c656028fb0b>\u001b[0m in \u001b[0;36mprocess_split\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCHUNK_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_WORKERS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_video\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'skipped'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mskipped_videos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mediapipe as mp\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed, TimeoutError\n",
        "\n",
        "# --- CONFIG ---\n",
        "BASE_DATASET_PATH = '/content/drive/MyDrive/ASL Project/dataset'\n",
        "BASE_SPLITS_PATH = '/content/drive/MyDrive/ASL Project/filtered splits'\n",
        "BASE_OUTPUT_PATH = '/content/drive/MyDrive/ASL Project/best_frames'\n",
        "SHARPNESS_THRESHOLD = 70.0\n",
        "SPLITS = ['train', 'val', 'test']\n",
        "NUM_WORKERS = 2  # Limit parallel workers to reduce memory use\n",
        "CHUNK_SIZE = 1000  # Process in smaller chunks\n",
        "TIMEOUT_PER_VIDEO = 7  # Timeout per video in seconds\n",
        "\n",
        "# --- SHARED PROCESSING FUNCTION ---\n",
        "def process_video(args):\n",
        "    video_folder, split, gloss_lookup = args\n",
        "\n",
        "    frames_path = os.path.join(BASE_DATASET_PATH, split, video_folder)\n",
        "    output_path = os.path.join(BASE_OUTPUT_PATH, split, f\"{video_folder}.jpg\")\n",
        "    dirty_path = os.path.join(BASE_OUTPUT_PATH, f'dirty_{split}', f\"{video_folder}.jpg\")\n",
        "\n",
        "    gloss = gloss_lookup.get(video_folder, \"\")\n",
        "\n",
        "    if os.path.exists(output_path) or os.path.exists(dirty_path):\n",
        "        return ('skipped', video_folder, gloss, None, None)\n",
        "\n",
        "    if not os.path.isdir(frames_path):\n",
        "        return ('missing', video_folder, gloss, None, None)\n",
        "\n",
        "    try:\n",
        "        mp_hands = mp.solutions.hands.Hands(static_image_mode=True, max_num_hands=2)\n",
        "        best_frame = None\n",
        "        best_score = -1\n",
        "\n",
        "        for frame_file in sorted(os.listdir(frames_path)):\n",
        "            frame_path = os.path.join(frames_path, frame_file)\n",
        "            try:\n",
        "                frame = cv2.imread(frame_path)\n",
        "                if frame is None:\n",
        "                    continue\n",
        "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "                sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                results = mp_hands.process(rgb)\n",
        "\n",
        "                if results.multi_hand_landmarks and sharpness > best_score:\n",
        "                    best_score = sharpness\n",
        "                    best_frame = frame\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing frame {frame_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        mp_hands.close()\n",
        "\n",
        "        if best_frame is None:\n",
        "            return ('dirty', video_folder, gloss, None, None)\n",
        "\n",
        "        if best_score >= SHARPNESS_THRESHOLD:\n",
        "            return ('clean', video_folder, gloss, best_score, best_frame)\n",
        "        else:\n",
        "            return ('dirty', video_folder, gloss, best_score, best_frame)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing video {video_folder}: {e}\")\n",
        "        return ('dirty', video_folder, gloss, None, None)\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "def process_split(split):\n",
        "    print(f\"\\n🚀 Processing split: {split}\")\n",
        "\n",
        "    output_dir = os.path.join(BASE_OUTPUT_PATH, split)\n",
        "    dirty_dir = os.path.join(BASE_OUTPUT_PATH, f'dirty_{split}')\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(dirty_dir, exist_ok=True)\n",
        "\n",
        "    csv_path = os.path.join(BASE_SPLITS_PATH, f'{split}_filtered.csv')\n",
        "    df = pd.read_csv(csv_path)\n",
        "    video_list = [os.path.splitext(v)[0] for v in df['Video file'].tolist()]\n",
        "    gloss_lookup = {os.path.splitext(row['Video file'])[0]: row['Gloss'] for _, row in df.iterrows()}\n",
        "    args = [(v, split, gloss_lookup) for v in video_list]\n",
        "\n",
        "    print(f\"📦 {len(args)} videos to process with {NUM_WORKERS} workers in chunks of {CHUNK_SIZE}\")\n",
        "\n",
        "    clean_entries = []\n",
        "    dirty_entries = []\n",
        "    missing_videos = []\n",
        "    skipped_videos = []\n",
        "    timeout_videos = []\n",
        "    sharpness_log_path = os.path.join(BASE_OUTPUT_PATH, f'{split}_sharpness.csv')\n",
        "\n",
        "    if os.path.exists(sharpness_log_path):\n",
        "        existing_sharpness_df = pd.read_csv(sharpness_log_path)\n",
        "        processed_videos = set(existing_sharpness_df['video'].tolist())\n",
        "    else:\n",
        "        processed_videos = set()\n",
        "\n",
        "    for i in range(0, len(args), CHUNK_SIZE):\n",
        "        chunk = args[i:i + CHUNK_SIZE]\n",
        "        with ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
        "            futures = {executor.submit(process_video, arg): arg for arg in chunk}\n",
        "            for future in tqdm(as_completed(futures, timeout=TIMEOUT_PER_VIDEO * len(chunk)), total=len(chunk)):\n",
        "                try:\n",
        "                    result = future.result(timeout=TIMEOUT_PER_VIDEO)\n",
        "                except TimeoutError:\n",
        "                    arg = futures[future]\n",
        "                    print(f\"⚠️ Timeout processing {arg[0]}\")\n",
        "                    timeout_videos.append(arg[0])\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    arg = futures[future]\n",
        "                    print(f\"❌ Crash processing {arg[0]}: {e}\")\n",
        "                    timeout_videos.append(arg[0])\n",
        "                    continue\n",
        "\n",
        "                if result[0] == 'skipped':\n",
        "                    skipped_videos.append(result[1])\n",
        "                    continue\n",
        "                elif result[0] == 'missing':\n",
        "                    missing_videos.append(result[1])\n",
        "                    continue\n",
        "\n",
        "                status, video_name, gloss, sharpness, frame = result\n",
        "                if status == 'dirty':\n",
        "                    if sharpness is not None:\n",
        "                        dirty_entries.append((video_name, gloss, sharpness))\n",
        "                    if frame is not None:\n",
        "                        cv2.imwrite(os.path.join(dirty_dir, f'{video_name}.jpg'), frame)\n",
        "                elif status == 'clean':\n",
        "                    clean_entries.append((video_name, gloss, sharpness))\n",
        "                    cv2.imwrite(os.path.join(output_dir, f'{video_name}.jpg'), frame)\n",
        "\n",
        "    all_entries = clean_entries + dirty_entries\n",
        "    all_df = pd.DataFrame(all_entries, columns=[\"video\", \"gloss\", \"sharpness\"])\n",
        "    if os.path.exists(sharpness_log_path):\n",
        "        prev_df = pd.read_csv(sharpness_log_path)\n",
        "        all_df = pd.concat([prev_df, all_df], ignore_index=True)\n",
        "\n",
        "    all_df = all_df.drop_duplicates(subset=\"video\").sort_values(by=\"sharpness\", ascending=False)\n",
        "    all_df.to_csv(sharpness_log_path, index=False)\n",
        "\n",
        "    with open(os.path.join(BASE_OUTPUT_PATH, 'missing_videos.txt'), 'a') as f:\n",
        "        for v in missing_videos:\n",
        "            f.write(f\"{v}\\n\")\n",
        "\n",
        "    with open(os.path.join(BASE_OUTPUT_PATH, 'dirty_videos.txt'), 'a') as f:\n",
        "        for v in [entry[0] for entry in dirty_entries if entry[0]]:\n",
        "            f.write(f\"{v}\\n\")\n",
        "\n",
        "    with open(os.path.join(BASE_OUTPUT_PATH, 'timeout_videos.txt'), 'a') as f:\n",
        "        for v in timeout_videos:\n",
        "            f.write(f\"{v}\\n\")\n",
        "\n",
        "    print(f\"✅ {split} done: {len(clean_entries)} clean, {len(dirty_entries)} dirty, {len(missing_videos)} missing, {len(skipped_videos)} skipped, {len(timeout_videos)} timeouts\")\n",
        "\n",
        "# --- RUN ALL SPLITS ---\n",
        "if __name__ == '__main__':\n",
        "    for split in SPLITS:\n",
        "        process_split(split)\n",
        "\n",
        "    print(\"\\n🎉 All splits completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A1uUrLMSJbO",
        "outputId": "62c79758-7169-4529-f88c-691537e1fd33"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 Processing split: train\n",
            "📦 21240 videos to process with 2 workers in chunks of 1000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [02:32<00:00,  6.56it/s]\n",
            "100%|██████████| 1000/1000 [00:15<00:00, 64.49it/s]\n",
            "100%|██████████| 1000/1000 [00:25<00:00, 39.98it/s]\n",
            "100%|██████████| 1000/1000 [00:39<00:00, 25.50it/s]\n",
            "100%|██████████| 1000/1000 [00:40<00:00, 24.47it/s]\n",
            "100%|██████████| 1000/1000 [00:56<00:00, 17.79it/s]\n",
            "100%|██████████| 1000/1000 [00:27<00:00, 35.74it/s]\n",
            "100%|██████████| 1000/1000 [01:17<00:00, 12.96it/s]\n",
            "100%|██████████| 1000/1000 [01:03<00:00, 15.86it/s]\n",
            "100%|██████████| 1000/1000 [00:16<00:00, 61.39it/s]\n",
            "100%|██████████| 1000/1000 [00:28<00:00, 35.55it/s]\n",
            "100%|██████████| 1000/1000 [00:42<00:00, 23.47it/s]\n",
            "100%|██████████| 1000/1000 [00:33<00:00, 30.04it/s]\n",
            "100%|██████████| 1000/1000 [00:32<00:00, 31.04it/s]\n",
            "100%|██████████| 1000/1000 [00:18<00:00, 53.45it/s]\n",
            "100%|██████████| 1000/1000 [00:21<00:00, 46.05it/s]\n",
            "100%|██████████| 1000/1000 [00:33<00:00, 29.81it/s]\n",
            "100%|██████████| 1000/1000 [01:31<00:00, 10.91it/s]\n",
            "100%|██████████| 1000/1000 [01:09<00:00, 14.49it/s]\n",
            "100%|██████████| 1000/1000 [00:30<00:00, 32.63it/s]\n",
            "100%|██████████| 1000/1000 [00:17<00:00, 55.99it/s]\n",
            "100%|██████████| 240/240 [00:02<00:00, 98.71it/s] \n",
            "<ipython-input-3-1f97b6259882>:141: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  all_df = pd.concat([prev_df, all_df], ignore_index=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ train done: 0 clean, 0 dirty, 2 missing, 20324 skipped, 0 timeouts\n",
            "\n",
            "🚀 Processing split: val\n",
            "📦 5446 videos to process with 2 workers in chunks of 1000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:19<00:00, 52.17it/s]\n",
            "100%|██████████| 1000/1000 [00:23<00:00, 41.98it/s]\n",
            "100%|██████████| 1000/1000 [00:29<00:00, 33.35it/s]\n",
            "100%|██████████| 1000/1000 [00:07<00:00, 133.50it/s]\n",
            "100%|██████████| 1000/1000 [00:33<00:00, 30.02it/s]\n",
            "100%|██████████| 446/446 [00:16<00:00, 27.56it/s]\n",
            "<ipython-input-3-1f97b6259882>:141: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  all_df = pd.concat([prev_df, all_df], ignore_index=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ val done: 0 clean, 0 dirty, 0 missing, 5273 skipped, 0 timeouts\n",
            "\n",
            "🚀 Processing split: test\n",
            "📦 17639 videos to process with 2 workers in chunks of 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:39<00:00, 25.31it/s]\n",
            "100%|██████████| 1000/1000 [00:23<00:00, 42.55it/s]\n",
            "100%|██████████| 1000/1000 [00:32<00:00, 30.40it/s]\n",
            "100%|██████████| 1000/1000 [00:44<00:00, 22.53it/s]\n",
            "100%|██████████| 1000/1000 [00:27<00:00, 35.78it/s]\n",
            "100%|██████████| 1000/1000 [00:11<00:00, 90.27it/s]\n",
            "100%|██████████| 1000/1000 [01:01<00:00, 16.13it/s]\n",
            "100%|██████████| 1000/1000 [10:49<00:00,  1.54it/s]\n",
            "100%|██████████| 1000/1000 [13:57<00:00,  1.19it/s]\n",
            "100%|██████████| 1000/1000 [13:01<00:00,  1.28it/s]\n",
            "100%|██████████| 1000/1000 [10:56<00:00,  1.52it/s]\n",
            "100%|██████████| 1000/1000 [10:28<00:00,  1.59it/s]\n",
            "100%|██████████| 1000/1000 [10:43<00:00,  1.55it/s]\n",
            "100%|██████████| 1000/1000 [07:16<00:00,  2.29it/s]\n",
            "100%|██████████| 1000/1000 [00:08<00:00, 113.23it/s]\n",
            "100%|██████████| 1000/1000 [00:08<00:00, 120.95it/s]\n",
            "100%|██████████| 1000/1000 [00:07<00:00, 138.54it/s]\n",
            "100%|██████████| 639/639 [00:05<00:00, 110.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ test done: 3419 clean, 2548 dirty, 3884 missing, 6860 skipped, 0 timeouts\n",
            "\n",
            "🎉 All splits completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rUANPhuAWkS",
        "outputId": "e5abdb1d-567d-429c-a1b5-8c3d5e351e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Usefulness of Sharpness Score"
      ],
      "metadata": {
        "id": "JnihR19q-Z74"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pwkz8GLC-fSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refiltering the Test Set"
      ],
      "metadata": {
        "id": "ZCZSEmyPkM4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === Step 2: Define paths\n",
        "base_path = '/content/drive/MyDrive/ASL Project/'\n",
        "test_image_dir = os.path.join(base_path, 'final_dataset/test')\n",
        "sharpness_csv = os.path.join(base_path, 'best_frames/test_sharpness.csv')\n",
        "discard_dir = os.path.join(base_path, 'best_frames/discarded_test')\n",
        "\n",
        "# Create discard folder if it doesn't exist\n",
        "os.makedirs(discard_dir, exist_ok=True)\n",
        "\n",
        "# === Step 3: Load sharpness scores\n",
        "df = pd.read_csv(sharpness_csv)\n",
        "\n",
        "# === Step 4: Sort by sharpness and keep top 4000\n",
        "top_k = 4000\n",
        "df_sorted = df.sort_values(by='sharpness', ascending=False)\n",
        "top_df = df_sorted.head(top_k)\n",
        "\n",
        "# Append '.jpg' to match actual filenames\n",
        "top_files = set(top_df['video'].astype(str).str.strip() + '.jpg')\n",
        "\n",
        "# Debug check\n",
        "print(f\"✅ Matching against {len(top_files)} top-scoring image filenames.\")\n",
        "\n",
        "\n",
        "# === Step 5: Refilter test folder based on top sharpness\n",
        "kept = 0\n",
        "moved = 0\n",
        "\n",
        "print(\"🚚 Filtering test set to top 4000 sharpest frames...\")\n",
        "for fname in tqdm(os.listdir(test_image_dir)):\n",
        "    src_path = os.path.join(test_image_dir, fname)\n",
        "    dst_path = os.path.join(discard_dir, fname)\n",
        "\n",
        "    if fname in top_files:\n",
        "        kept += 1\n",
        "    else:\n",
        "        if os.path.isfile(src_path):\n",
        "            shutil.move(src_path, dst_path)\n",
        "            moved += 1\n",
        "\n",
        "print(f\"\\n✅ Done. Kept {kept} images with highest sharpness.\")\n",
        "print(f\"🗑️ Moved {moved} images to discarded_test.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyRe2fzckQG2",
        "outputId": "cf7a5b6b-3b65-4437-96b8-bed8a840d01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Matching against 4000 top-scoring image filenames.\n",
            "🚚 Filtering test set to top 4000 sharpest frames...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7760/7760 [00:16<00:00, 482.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Done. Kept 3423 images with highest sharpness.\n",
            "🗑️ Moved 4337 images to discarded_test.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensuring that the Test and Val sets are subsets of Train"
      ],
      "metadata": {
        "id": "XSG3ZcChmVci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === Step 2: Define paths\n",
        "base_path = '/content/drive/MyDrive/ASL Project/final_dataset/'\n",
        "discard_path = '/content/drive/MyDrive/ASL Project/'\n",
        "\n",
        "train_dir = os.path.join(base_path, 'train')\n",
        "val_dir = os.path.join(base_path, 'val')\n",
        "test_dir = os.path.join(base_path, 'test')\n",
        "\n",
        "discard_val = os.path.join(discard_path, 'discarded_val')\n",
        "discard_test = os.path.join(discard_path, 'discarded_test')\n",
        "\n",
        "os.makedirs(discard_val, exist_ok=True)\n",
        "os.makedirs(discard_test, exist_ok=True)\n",
        "\n",
        "# === Step 3: Extract gloss from filename\n",
        "def extract_cleaned_gloss(filename):\n",
        "    try:\n",
        "        # Extract the part after '-' and before '_' or '.jpg'\n",
        "        base = filename.split('_')[0]           # e.g. '123456789-GLASS 3'\n",
        "        gloss = base.split('-')[1]              # 'GLASS 3'\n",
        "\n",
        "        # Remove trailing space + digits or trailing digits\n",
        "        gloss = re.sub(r'\\s*\\d+$', '', gloss)   # 'GLASS 3' -> 'GLASS', 'GLASS3' -> 'GLASS'\n",
        "\n",
        "        # Optional: normalize internal spacing\n",
        "        gloss = gloss.strip().upper()\n",
        "\n",
        "        return gloss\n",
        "    except IndexError:\n",
        "        print(f\"⚠️ Could not extract gloss from: {filename}\")\n",
        "        return None\n",
        "\n",
        "# === Step 4: Build allowed gloss set from train/\n",
        "train_glosses = set()\n",
        "for fname in os.listdir(train_dir):\n",
        "    if os.path.isfile(os.path.join(train_dir, fname)):\n",
        "        gloss = extract_cleaned_gloss(fname)\n",
        "        if gloss:\n",
        "            train_glosses.add(gloss)\n",
        "\n",
        "print(f\"✅ Found {len(train_glosses)} unique glosses in train set.\")\n",
        "\n",
        "# === Step 5: Function to filter folder\n",
        "def filter_folder(folder, discard_folder, name):\n",
        "    kept = 0\n",
        "    total = 0\n",
        "    for fname in tqdm(os.listdir(folder), desc=f\"Filtering {name}\"):\n",
        "        file_path = os.path.join(folder, fname)\n",
        "        if not os.path.isfile(file_path):\n",
        "            continue\n",
        "        total += 1\n",
        "        gloss = extract_cleaned_gloss(fname)\n",
        "        if gloss in train_glosses:\n",
        "            kept += 1\n",
        "        else:\n",
        "            shutil.move(file_path, os.path.join(discard_folder, fname))\n",
        "    print(f\"📂 {name}: Kept {kept}/{total} images.\")\n",
        "\n",
        "# === Step 6: Apply to val/ and test/\n",
        "filter_folder(val_dir, discard_val, 'val')\n",
        "filter_folder(test_dir, discard_test, 'test')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdF5TNKGmZDz",
        "outputId": "a36e13d9-2f92-4865-f8b2-6065a858bd96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Found 2787 unique glosses in train set.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Filtering val: 100%|██████████| 3957/3957 [00:00<00:00, 7966.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 val: Kept 3957/3957 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Filtering test: 100%|██████████| 7760/7760 [00:00<00:00, 8169.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 test: Kept 7760/7760 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}