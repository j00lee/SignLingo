# -*- coding: utf-8 -*-
"""Comp646TextToGlossFinalVersion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OR8ollAboFetjvmk_TfXnQJ4spmT4Ukz
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')
aslg_ec23_file_path = '/content/drive/My Drive/ASLG_EC23.csv'
df_aslg_pc12 = pd.read_parquet("hf://datasets/achrafothman/aslg_pc12/data/train-00000-of-00001.parquet")
df_aslg_ec23 = pd.read_csv(aslg_ec23_file_path)

df_aslg_pc12 = df_aslg_pc12.drop_duplicates()
df_aslg_ec23 = df_aslg_ec23.drop_duplicates()

#Removing the '\n' from the gloss and text columns
df_aslg_pc12['gloss'] = df_aslg_pc12['gloss'].str.replace('\n', '')
df_aslg_pc12['text'] = df_aslg_pc12['text'].str.replace('\n', '')

#converting the gloss and text columns to lowercase
df_aslg_pc12['gloss'] = df_aslg_pc12['gloss'].str.lower()
df_aslg_pc12['text']  = df_aslg_pc12['text'].str.lower()

df_aslg_ec23['input'] = df_aslg_ec23['input'].str.lower()
df_aslg_ec23['target'] = df_aslg_ec23['target'].str.lower()

#Renaming the columns -- "gloss" to "Gloss", "text" to "English" for aslg_pc12, "input" to "Gloss" and "target" to "English" for alsg_ec23

df_aslg_pc12 = df_aslg_pc12.rename(mapper = {"gloss": "Gloss", "text": "English"}, axis = 1)
df_aslg_ec23 = df_aslg_ec23.rename(mapper = {"input": "Gloss", "target": "English"}, axis = 1)
df_aslg_pc12.head(10)
df_aslg_ec23.head(10)

"""**Metrics**

BLEU - Compares n-grams of the text generation/translation to the n-grams of the reference text(s).
      

*   Uses modified n-gram precision
   *   ex. modified unigram precision
       *    Clip(Number of word matches) / (Number of words generated)
       *    Clip(Number of word matches) sets an upper bound on a
       particular word's contribution to the numerator -- ex. if a word only appears once in the reference translation it can only increment the numerator by one maximum -- any other times it occurs in our generated translation won't increase the score
*  We will be using SacreBLEU, as it does not expect the text to be tokenized already, and is more comparable with other models.
* SacreBLEU expects a list of strings for its predictions parameter, and a list of lists of strings for its reference parameter (only one element within each sublist for our usecase at the moment since we only have one reference ground truth translation for each english phrase)




"""

!pip install sacrebleu
!pip install evaluate


# testing sacreblue
import evaluate

sacrebleu = evaluate.load("sacrebleu")
predictions = ["hello there general kenobi",
                "on our way to ankh morpork"]
references = [["hello there general kenobi"],
                 ["goodbye ankh morpork"]]
results = sacrebleu.compute(predictions = predictions, references = references)
print(results)





from huggingface_hub import notebook_login
notebook_login()

# Loading our t5 tokenizer

from transformers import AutoTokenizer

# t5-small checkpoint and tokenizer
t5_small_checkpoint = "google-t5/t5-small"
t5_small_tokenizer = AutoTokenizer.from_pretrained(t5_small_checkpoint)

# t5-base checkpoint and tokenizer
t5_base_checkpoint = "google-t5/t5-base"
t5_base_tokenizer = AutoTokenizer.from_pretrained(t5_base_checkpoint)

# bart-base checkpoint and tokenizer
bart_base_checkpoint = "facebook/bart-base"
bart_base_tokenizer = AutoTokenizer.from_pretrained(bart_base_checkpoint)

# bart-large checkpoint and tokenizer
bart_large_checkpoint = "facebook/bart-large"
bart_large_tokenizer = AutoTokenizer.from_pretrained(bart_large_checkpoint)

# Setting seed for deterministic output
import os
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"

import random
import numpy as np
import torch

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # if using multi-GPU

    # For deterministic behavior
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    # Also might be useful if using DataLoader workers
    torch.use_deterministic_algorithms(True)

set_seed(42)

"""**Defining our preprocessing function**

*Preprocessing function requirements*


1.   Must prefix input with a given prompt so that the T5 recognizes that we are performing a translation task. Prefix is "translate {source_language} to {target_language}: ".
2.   Must set the target language (in our case Gloss) to ensure the tokenizer processes the target text correctly.
3.   Must truncate sequences to be shorter than the max_length parameter


"""

from datasets import Dataset

#Changing our small subset df to hugging face dataset format
# dataset_initial_subset = Dataset.from_pandas(df_initial_subset)
aslg_pc12_dataset = Dataset.from_pandas(df_aslg_pc12)
aslg_ec23_dataset = Dataset.from_pandas(df_aslg_ec23)

train_aslg_pc12_dataset, val_aslg_pc12_dataset = aslg_pc12_dataset.train_test_split(test_size = 0.2).values()
val_aslg_pc12_dataset, test_aslg_pc12_dataset = val_aslg_pc12_dataset.train_test_split(test_size = 0.5).values()

train_aslg_ec23_dataset, val_aslg_ec23_dataset = aslg_ec23_dataset.train_test_split(test_size = 0.2).values()
val_aslg_ec23_dataset, test_aslg_ec23_dataset = val_aslg_ec23_dataset.train_test_split(test_size = 0.5).values()


source_lang = "English"
target_lang = "Gloss"
prefix = "translate English to Gloss: "

def get_preprocess_function(tokenizer, model_type = "t5", prefix = "translate English to Gloss: "):
  def preprocess_function(examples):
    if model_type == "t5":
        inputs = [prefix + text for text in examples["English"]]
    else: # BART does not require a prefix
        inputs = examples["English"]
    targets = examples[target_lang]
    model_inputs = tokenizer(inputs, text_target = targets, max_length = 128, truncation = True, padding = True)
    return model_inputs
  return preprocess_function

t5_small_preprocess_function = get_preprocess_function(tokenizer = t5_small_tokenizer, model_type = "t5", prefix = prefix)
t5_base_preprocess_function = get_preprocess_function(tokenizer = t5_base_tokenizer, model_type = "t5", prefix = prefix)
bart_base_preprocess_function = get_preprocess_function(tokenizer = bart_base_tokenizer, model_type = "bart", prefix = prefix)
bart_large_preprocess_function = get_preprocess_function(tokenizer = bart_large_tokenizer, model_type = "bart", prefix = prefix)
# Datasets provides a map function to apply this preprocessing function over the entire dataset,
# which can be sped up by setting batched = True to use batch processing

t5_small_tokenized_train_aslg_pc12_dataset = train_aslg_pc12_dataset.map(t5_small_preprocess_function, batched = True)
t5_small_tokenized_val_aslg_pc12_dataset = val_aslg_pc12_dataset.map(t5_small_preprocess_function, batched = True)
t5_small_tokenized_test_aslg_pc12_dataset = test_aslg_pc12_dataset.map(t5_small_preprocess_function, batched = True)

t5_small_tokenized_train_aslg_ec23_dataset = train_aslg_ec23_dataset.map(t5_small_preprocess_function, batched = True)
t5_small_tokenized_val_aslg_ec23_dataset = val_aslg_ec23_dataset.map(t5_small_preprocess_function, batched = True)
t5_small_tokenized_test_aslg_ec23_dataset = test_aslg_ec23_dataset.map(t5_small_preprocess_function, batched = True)


t5_base_tokenized_train_aslg_pc12_dataset = train_aslg_pc12_dataset.map(t5_base_preprocess_function, batched = True)
t5_base_tokenized_val_aslg_pc12_dataset = val_aslg_pc12_dataset.map(t5_base_preprocess_function, batched = True)
t5_base_tokenized_test_aslg_pc12_dataset = test_aslg_pc12_dataset.map(t5_base_preprocess_function, batched = True)

t5_base_tokenized_train_aslg_ec23_dataset = train_aslg_ec23_dataset.map(t5_base_preprocess_function, batched = True)
t5_base_tokenized_val_aslg_ec23_dataset = val_aslg_ec23_dataset.map(t5_base_preprocess_function, batched = True)
t5_base_tokenized_test_aslg_ec23_dataset = test_aslg_ec23_dataset.map(t5_base_preprocess_function, batched = True)



bart_base_tokenized_train_aslg_pc12_dataset = train_aslg_pc12_dataset.map(bart_base_preprocess_function, batched = True)
bart_base_tokenized_val_aslg_pc12_dataset = val_aslg_pc12_dataset.map(bart_base_preprocess_function, batched = True)
bart_base_tokenized_test_aslg_pc12_dataset = test_aslg_pc12_dataset.map(bart_base_preprocess_function, batched = True)

bart_base_tokenized_train_aslg_ec23_dataset = train_aslg_ec23_dataset.map(bart_base_preprocess_function, batched = True)
bart_base_tokenized_val_aslg_ec23_dataset = val_aslg_ec23_dataset.map(bart_base_preprocess_function, batched = True)
bart_base_tokenized_test_aslg_ec23_dataset = test_aslg_ec23_dataset.map(bart_base_preprocess_function, batched = True)


bart_large_tokenized_train_aslg_pc12_dataset = train_aslg_pc12_dataset.map(bart_large_preprocess_function, batched = True)
bart_large_tokenized_val_aslg_pc12_dataset =  val_aslg_pc12_dataset.map(bart_large_preprocess_function, batched = True)
bart_large_tokenized_test_aslg_pc12_dataset = test_aslg_pc12_dataset.map(bart_large_preprocess_function, batched = True)

bart_large_tokenized_train_aslg_ec23_dataset = train_aslg_ec23_dataset.map(bart_large_preprocess_function, batched = True)
bart_large_tokenized_val_aslg_ec23_dataset = val_aslg_ec23_dataset.map(bart_large_preprocess_function, batched = True)
bart_large_tokenized_test_aslg_ec23_dataset = test_aslg_ec23_dataset.map(bart_large_preprocess_function, batched = True)

import re

def clean_and_tokenize(text, is_gloss=False):
    text = text.lower()

    # Remove punctuation
    text = re.sub(r"[^\w\s]", "", text)

    if is_gloss:
        # Remove gloss-specific suffixes like ++, --, IX, etc.
        text = re.sub(r"\b([a-z]+)(\+\+|--|-ix)?\b", r"\1", text)

    tokens = text.strip().split()
    return tokens


def get_clean_vocab(dataset, field, is_gloss=False):
    vocab = set()
    for example in dataset:
        if field in example:
            tokens = clean_and_tokenize(example[field], is_gloss=is_gloss)
            vocab.update(tokens)
    return vocab

# Get vocab sizes for each split
datasets = {
    "pc12_train": train_aslg_pc12_dataset,
    "pc12_val": val_aslg_pc12_dataset,
    "pc12_test": test_aslg_pc12_dataset,
    "ec23_train": train_aslg_ec23_dataset,
    "ec23_val": val_aslg_ec23_dataset,
    "ec23_test": test_aslg_ec23_dataset,
}

for name, dataset in datasets.items():
    eng_vocab = get_clean_vocab(dataset, "English", is_gloss=False)
    gloss_vocab = get_clean_vocab(dataset, "Gloss", is_gloss=True)
    print(f"{name}: English={len(eng_vocab)}, Gloss={len(gloss_vocab)}")

from transformers import DataCollatorForSeq2Seq

#using a data collator to dynamically pad batches to longest batch length
t5_small_data_collator = DataCollatorForSeq2Seq(tokenizer = t5_small_tokenizer, model = t5_small_checkpoint)
t5_base_data_collator = DataCollatorForSeq2Seq(tokenizer = t5_base_tokenizer, model = t5_base_checkpoint)
bart_base_data_collator = DataCollatorForSeq2Seq(tokenizer = bart_base_tokenizer, model = bart_base_checkpoint)
bart_large_data_collator = DataCollatorForSeq2Seq(tokenizer = bart_large_tokenizer, model = bart_large_checkpoint)

"""Creating Evaluation Methods to compute SacreBLEU"""

import numpy as np
from evaluate import load
!pip install rouge_score

sacrebleu = load("sacrebleu")
rouge = load("rouge")
meteor = load("meteor")
chrf = load("chrf")

def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]
    return preds, labels

def compute_metrics(eval_preds, tokenizer):
    preds, labels = eval_preds

    if isinstance(preds[0], str):
        decoded_preds = preds
        decoded_labels = labels
    else:
        if isinstance(preds, tuple):
            preds = preds[0]
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    # Compute all metrics
    sacrebleu_result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)
    rouge_result = rouge.compute(predictions=decoded_preds, references=[label[0] for label in decoded_labels], rouge_types=["rougeL"])
    meteor_result = meteor.compute(predictions=decoded_preds, references=[label[0] for label in decoded_labels])
    chrf_result = chrf.compute(predictions=decoded_preds, references=decoded_labels)

    result = {
        "sacrebleu": sacrebleu_result["score"],
        "rougeL": rouge_result["rougeL"],
        "meteor": meteor_result["meteor"],
        "chrf": chrf_result["score"],
    }

    result = {k: round(v, 4) for k, v in result.items()}
    return result

"""Load our T5 Model"""

from transformers import AutoModelForSeq2SeqLM

# t5_small_
# t5_s = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

import wandb
!wandb login b3a52460a9c60505f183954134c8ff6a08afb1f3

"""Create our training method"""

!pip install livelossplot
# !pip install torch
import torch
import os
from livelossplot import PlotLosses
from torch.cuda.amp import autocast, GradScaler


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def train_english_to_gloss_model(model,
                               tokenizer,
                               data_collator,
                               train_loader,
                               val_loader,
                               num_epochs,
                               optimizer,
                               scheduler,
                               checkpoint_dir,
                               run_name=None,  # Add run_name parameter
                               cost_function = torch.nn.CrossEntropyLoss(),
                               device = torch.device("cuda" if torch.cuda.is_available() else "cpu"),
                               save_best_only = True,
                               save_freq = 3,
                               compute_metrics = compute_metrics,
                               liveloss = PlotLosses(),
                               precision = "bf16"
                               ):
  # Generate a timestamp for unique run identification if run_name not provided
  if run_name is None:
    import datetime
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"english_to_gloss_{timestamp}"

  # Initialize wandb with the unique run name
  wandb.init(
    project="Comp646EnglishToGloss",
    entity="aa270-rice-university",
    name=run_name,  # Use the unique run name
    reinit = True,
    config={
      "epochs": num_epochs,
      "batch_size": train_loader.batch_size if hasattr(train_loader, "batch_size") else "unknown",
      "precision": precision,
      "optimizer": optimizer.__class__.__name__,
      "scheduler": scheduler.__class__.__name__ if scheduler else "None"
    }
  )

  model = model.to(device)
  best_loss = float("inf")
  os.makedirs(checkpoint_dir, exist_ok = True)
  cost_function = cost_function.to(device)

  use_amp = precision == "bf16" or precision == "fp16"
  dtype = torch.bfloat16 if precision == "bf16" else torch.float16 if precision == "fp16" else torch.float32
  scaler = GradScaler(enabled = (precision == "fp16"))

  for epoch in range(num_epochs):
    print(f"Epoch {epoch + 1} / {num_epochs}")
    logs = {}

    model.train()
    cumulative_train_loss = 0.0
    num_train_batches = 0

    train_preds = []
    train_labels = []
    for batch_id, batch in enumerate(train_loader):
      print(f"Training batch {batch_id + 1}/{len(train_loader)}")

      xb = batch['input_ids'].to(device)
      yb = batch['labels'].to(device)

      optimizer.zero_grad()
      with autocast(dtype=dtype, enabled=use_amp):
        predicted = model(input_ids=xb, labels=yb)
        loss = predicted.loss

      if use_amp:
        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()
      else:
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

      cumulative_train_loss += loss.item()
      num_train_batches += 1

    avg_train_loss = cumulative_train_loss / num_train_batches
    logs['train_loss'] = avg_train_loss

    model.eval()

    num_val_batches = 0
    val_preds = []
    val_labels = []
    cumulative_val_loss = 0.0

    with torch.no_grad():
      for batch_id, batch in enumerate(val_loader):
        print(f"Validation batch {batch_id + 1}/{len(val_loader)}")

        xb = batch['input_ids'].to(device)
        yb = batch['labels'].to(device)

        predicted = model(input_ids = xb, labels = yb)
        loss = predicted.loss

        cumulative_val_loss += loss.item()
        num_val_batches += 1

        generated_ids = model.generate(input_ids = xb,
                                     max_length = 128)

        decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens = True)
        # Replace -100 with pad token ID for decoding
        labels = np.where(yb.cpu().numpy() != -100, yb.cpu().numpy(), tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens = True)

        # Store the decoded strings from val set
        val_preds.extend(decoded_preds)
        val_labels.extend(decoded_labels)

    avg_val_loss = cumulative_val_loss / num_val_batches
    logs['val_loss'] = avg_val_loss

    val_metrics = compute_metrics((val_preds, val_labels), tokenizer)
    logs.update(val_metrics)
    print(f"Train Loss: {avg_train_loss:.4f}")
    print(f"Val Loss: {avg_val_loss:.4f}")
    for key, value in val_metrics.items():
      print(f"{key}: {value:.4f}")

    # Save model if it's the best one so far
    if save_best_only:
      if logs['val_loss'] < best_loss:
        best_loss = logs['val_loss']
        # Create a unique checkpoint directory for each run
        unique_checkpoint_dir = os.path.join(checkpoint_dir, run_name)
        os.makedirs(unique_checkpoint_dir, exist_ok=True)
        model.save_pretrained(unique_checkpoint_dir)
        tokenizer.save_pretrained(unique_checkpoint_dir)
        print(f"Saved best model to {unique_checkpoint_dir}")

    liveloss.update(logs)
    liveloss.draw()

    wandb.log(logs)

    if scheduler is not None:
      scheduler.step()

  wandb.finish()


  return model

"""## BART LARGE ASLG PC12"""

train_english_to_gloss_model(
    model = bart_large_model,
    tokenizer = bart_large_tokenizer,
    data_collator = bart_large_data_collator,
    train_loader = train_aslg_pc12_loader,
    val_loader = val_aslg_pc12_loader,

)

from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import torch
# checkpoint = "google-t5/t5-small"
# t5_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
# if hasattr(torch, "compile") and callable(torch.compile):
#     t5_model = torch.compile(t5_model)
# tokenizer = AutoTokenizer.from_pretrained(checkpoint)
# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=t5_model, return_tensors="pt")

# # if any(col in tokenized_train_set_initial_subset.column_names for col in ['English', "Gloss", "__index_level_0__"]):
# #   tokenized_train_set_initial_subset = tokenized_train_set_initial_subset.remove_columns(["English", "Gloss", "__index_level_0__"])
# # if any(col in tokenized_test_set_initial_subset.column_names for col in ['English', "Gloss", "__index_level_0__"]):
# #   tokenized_test_set_initial_subset = tokenized_test_set_initial_subset.remove_columns(["English", "Gloss", "__index_level_0__"])

# def filter_columns(dataset):
#   columns_to_remove = ['English', "Gloss", "__index_level_0__"]
#   for column in columns_to_remove:
#     if column in dataset.column_names:
#       dataset = dataset.remove_columns(column)
#   return dataset

# tokenized_train_aslg_pc12_dataset = filter_columns(tokenized_train_aslg_pc12_dataset)
# tokenized_val_aslg_pc12_dataset = filter_columns(tokenized_val_aslg_pc12_dataset)
# tokenized_test_aslg_pc12_dataset = filter_columns(tokenized_test_aslg_pc12_dataset)

# tokenized_train_aslg_ec23_dataset = filter_columns(tokenized_train_aslg_ec23_dataset)
# tokenized_val_aslg_ec23_dataset = filter_columns(tokenized_val_aslg_ec23_dataset)
# tokenized_test_aslg_ec23_dataset =filter_columns( tokenized_test_aslg_ec23_dataset)

# train_aslg_pc12_loader = DataLoader(tokenized_train_aslg_pc12_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)
# val_aslg_pc12_loader = DataLoader(tokenized_val_aslg_pc12_dataset, batch_size=32, shuffle=False, collate_fn=data_collator)
# test_aslg_pc12_loader = DataLoader(tokenized_test_aslg_pc12_dataset, batch_size=32, shuffle=False, collate_fn=data_collator)

# train_aslg_ec23_loader = DataLoader(tokenized_train_aslg_ec23_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)
# val_aslg_ec23_loader = DataLoader(tokenized_val_aslg_ec23_dataset, batch_size=32, shuffle=False, collate_fn=data_collator)
# test_aslg_ec23_loader = DataLoader(tokenized_test_aslg_ec23_dataset, batch_size=32, shuffle=False, collate_fn=data_collator)



# num_epochs = 5
# learning_rate = 3e-4
# weight_decay = 1e-3
# optimizer = AdamW(t5_model.parameters(), lr=learning_rate, weight_decay=weight_decay)
# scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)
# checkpoint_dir = "t5_model_english_to_gloss_aslg_pc12_v1"
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# cost_function = torch.nn.CrossEntropyLoss()

# checkpoint = "google-t5/t5-small"
# t5_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
# if hasattr(torch, "compile") and callable(torch.compile):
#     t5_model = torch.compile(t5_model)
# tokenizer = AutoTokenizer.from_pretrained(checkpoint)
# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=t5_model, return_tensors="pt")

# if any(col in tokenized_train_set_initial_subset.column_names for col in ['English', "Gloss", "__index_level_0__"]):
#   tokenized_train_set_initial_subset = tokenized_train_set_initial_subset.remove_columns(["English", "Gloss", "__index_level_0__"])
# if any(col in tokenized_test_set_initial_subset.column_names for col in ['English', "Gloss", "__index_level_0__"]):
#   tokenized_test_set_initial_subset = tokenized_test_set_initial_subset.remove_columns(["English", "Gloss", "__index_level_0__"])

def filter_columns(dataset):
  columns_to_remove = ['English', "Gloss", "__index_level_0__"]
  for column in columns_to_remove:
    if column in dataset.column_names:
      dataset = dataset.remove_columns(column)
  return dataset


def load_model_and_tokenizer(checkpoint: str):
    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
    if hasattr(torch, "compile") and callable(torch.compile):
        model = torch.compile(model)
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    return model.to(DEVICE), tokenizer

def filter_columns(dataset, columns_to_remove=["English", "Gloss", "__index_level_0__"]):
    return dataset.remove_columns([col for col in columns_to_remove if col in dataset.column_names])

def create_dataloader(dataset, tokenizer, model, batch_size=32, shuffle=False):
    dataset = filter_columns(dataset)
    collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors="pt")
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collator)

def build_all_dataloaders(tokenized_datasets: dict, tokenizer, model, batch_size=32):
    loaders = {}
    for name, dataset in tokenized_datasets.items():
        shuffle = "train" in name
        loaders[name] = create_dataloader(dataset, tokenizer, model, batch_size, shuffle=shuffle)
    return loaders

def prepare_optimizer_scheduler(model, num_epochs, learning_rate, weight_decay):
    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)
    return optimizer, scheduler


# For t5_small
t5_small_tokenized_train_aslg_pc12_dataset = filter_columns(t5_small_tokenized_train_aslg_pc12_dataset)
t5_small_tokenized_val_aslg_pc12_dataset   = filter_columns(t5_small_tokenized_val_aslg_pc12_dataset)
t5_small_tokenized_test_aslg_pc12_dataset  = filter_columns(t5_small_tokenized_test_aslg_pc12_dataset)

t5_small_tokenized_train_aslg_ec23_dataset = filter_columns(t5_small_tokenized_train_aslg_ec23_dataset)
t5_small_tokenized_val_aslg_ec23_dataset   = filter_columns(t5_small_tokenized_val_aslg_ec23_dataset)
t5_small_tokenized_test_aslg_ec23_dataset  = filter_columns(t5_small_tokenized_test_aslg_ec23_dataset)

# For t5_base
t5_base_tokenized_train_aslg_pc12_dataset  = filter_columns(t5_base_tokenized_train_aslg_pc12_dataset)
t5_base_tokenized_val_aslg_pc12_dataset    = filter_columns(t5_base_tokenized_val_aslg_pc12_dataset)
t5_base_tokenized_test_aslg_pc12_dataset   = filter_columns(t5_base_tokenized_test_aslg_pc12_dataset)

t5_base_tokenized_train_aslg_ec23_dataset  = filter_columns(t5_base_tokenized_train_aslg_ec23_dataset)
t5_base_tokenized_val_aslg_ec23_dataset    = filter_columns(t5_base_tokenized_val_aslg_ec23_dataset)
t5_base_tokenized_test_aslg_ec23_dataset   = filter_columns(t5_base_tokenized_test_aslg_ec23_dataset)

# For bart_base
bart_base_tokenized_train_aslg_pc12_dataset = filter_columns(bart_base_tokenized_train_aslg_pc12_dataset)
bart_base_tokenized_val_aslg_pc12_dataset   = filter_columns(bart_base_tokenized_val_aslg_pc12_dataset)
bart_base_tokenized_test_aslg_pc12_dataset  = filter_columns(bart_base_tokenized_test_aslg_pc12_dataset)

bart_base_tokenized_train_aslg_ec23_dataset = filter_columns(bart_base_tokenized_train_aslg_ec23_dataset)
bart_base_tokenized_val_aslg_ec23_dataset   = filter_columns(bart_base_tokenized_val_aslg_ec23_dataset)
bart_base_tokenized_test_aslg_ec23_dataset  = filter_columns(bart_base_tokenized_test_aslg_ec23_dataset)

# For bart_large
bart_large_tokenized_train_aslg_pc12_dataset = filter_columns(bart_large_tokenized_train_aslg_pc12_dataset)
bart_large_tokenized_val_aslg_pc12_dataset   = filter_columns(bart_large_tokenized_val_aslg_pc12_dataset)
bart_large_tokenized_test_aslg_pc12_dataset  = filter_columns(bart_large_tokenized_test_aslg_pc12_dataset)

bart_large_tokenized_train_aslg_ec23_dataset = filter_columns(bart_large_tokenized_train_aslg_ec23_dataset)
bart_large_tokenized_val_aslg_ec23_dataset   = filter_columns(bart_large_tokenized_val_aslg_ec23_dataset)
bart_large_tokenized_test_aslg_ec23_dataset  = filter_columns(bart_large_tokenized_test_aslg_ec23_dataset)
# num_epochs = 5
# learning_rate = 3e-4
# weight_decay = 1e-3
# optimizer = AdamW(t5_model.parameters(), lr=learning_rate, weight_decay=weight_decay)
# scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)
# checkpoint_dir = "t5_model_english_to_gloss_aslg_pc12_v1"
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# cost_function = torch.nn.CrossEntropyLoss()

"""## T5 SMALL SETUP"""

# === Config ===
T5_SMALL_BATCH_SIZE = 128
T5_SMALL_ASLG_PC12_NUM_EPOCHS = 5
T5_SMALL_ASLG_PC12_LEARNING_RATE = 3e-4
T5_SMALL_ASLG_PC12_WEIGHT_DECAY = 1e-3

T5_SMALL_ASLG_EC23_NUM_EPOCHS = 35
T5_SMALL_ASLG_EC23_LEARNING_RATE = 3e-4
T5_SMALL_ASLG_EC23_WEIGHT_DECAY = 1e-3
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


t5_small_checkpoint = "google-t5/t5-small"
t5_small_model, t5_small_tokenizer = load_model_and_tokenizer(t5_small_checkpoint)

t5_small_tokenized_datasets = {
    "pc12_train": t5_small_tokenized_train_aslg_pc12_dataset,
    "pc12_val": t5_small_tokenized_val_aslg_pc12_dataset,
    "pc12_test": t5_small_tokenized_test_aslg_pc12_dataset,
    "ec23_train": t5_small_tokenized_train_aslg_ec23_dataset,
    "ec23_val": t5_small_tokenized_val_aslg_ec23_dataset,
    "ec23_test": t5_small_tokenized_test_aslg_ec23_dataset,
}

t5_small_loaders = build_all_dataloaders(t5_small_tokenized_datasets, t5_small_tokenizer, t5_small_model, T5_SMALL_BATCH_SIZE)
t5_small_aslg_pc12_optimizer, t5_small_aslg_pc12_scheduler = prepare_optimizer_scheduler(t5_small_model, T5_SMALL_ASLG_PC12_NUM_EPOCHS, T5_SMALL_ASLG_PC12_LEARNING_RATE, T5_SMALL_ASLG_PC12_WEIGHT_DECAY)
t5_small_aslg_ec23_optimizer, t5_small_aslg_ec23_scheduler = prepare_optimizer_scheduler(t5_small_model, T5_SMALL_ASLG_EC23_NUM_EPOCHS, T5_SMALL_ASLG_EC23_LEARNING_RATE, T5_SMALL_ASLG_EC23_WEIGHT_DECAY)

"""## T5 BASE SETUP

"""

T5_BASE_BATCH_SIZE = 64
T5_BASE_ASLG_PC12_NUM_EPOCHS = 5
T5_BASE_ASLG_PC12_LEARNING_RATE = 3e-4
T5_BASE_ASLG_PC12_WEIGHT_DECAY = 1e-3

T5_BASE_ASLG_EC23_NUM_EPOCHS = 35
T5_BASE_ASLG_EC23_LEARNING_RATE = 3e-4
T5_BASE_ASLG_EC23_WEIGHT_DECAY = 1e-3
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

t5_base_checkpoint = "google-t5/t5-base"
t5_base_model, t5_base_tokenizer = load_model_and_tokenizer(t5_base_checkpoint)

t5_base_tokenized_datasets = {
    "pc12_train": t5_base_tokenized_train_aslg_pc12_dataset,
    "pc12_val": t5_base_tokenized_val_aslg_pc12_dataset,
    "pc12_test": t5_base_tokenized_test_aslg_pc12_dataset,
    "ec23_train": t5_base_tokenized_train_aslg_ec23_dataset,
    "ec23_val": t5_base_tokenized_val_aslg_ec23_dataset,
    "ec23_test": t5_base_tokenized_test_aslg_ec23_dataset,
}

t5_base_loaders = build_all_dataloaders(t5_base_tokenized_datasets, t5_base_tokenizer, t5_base_model, T5_BASE_BATCH_SIZE)
t5_base_aslg_pc12_optimizer, t5_base_aslg_pc12_scheduler = prepare_optimizer_scheduler(t5_base_model, T5_BASE_ASLG_PC12_NUM_EPOCHS, T5_BASE_ASLG_PC12_LEARNING_RATE, T5_BASE_ASLG_PC12_WEIGHT_DECAY)
t5_base_aslg_ec23_optimizer, t5_base_aslg_ec23_scheduler = prepare_optimizer_scheduler(t5_base_model, T5_BASE_ASLG_EC23_NUM_EPOCHS, T5_BASE_ASLG_EC23_LEARNING_RATE, T5_BASE_ASLG_EC23_WEIGHT_DECAY)

"""## BART BASE SETUP"""

BART_BASE_BATCH_SIZE = 64
BART_BASE_ASLG_PC12_NUM_EPOCHS = 6
BART_BASE_ASLG_PC12_LEARNING_RATE = 3e-4
BART_BASE_ASLG_PC12_WEIGHT_DECAY = 1e-3

BART_BASE_ASLG_EC23_NUM_EPOCHS = 40
BART_BASE_ASLG_EC23_LEARNING_RATE = 3e-4
BART_BASE_ASLG_EC23_WEIGHT_DECAY = 1e-5
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

bart_base_checkpoint = "facebook/bart-base"
bart_base_model, bart_base_tokenizer = load_model_and_tokenizer(bart_base_checkpoint)

bart_base_tokenized_datasets = {
    "pc12_train": bart_base_tokenized_train_aslg_pc12_dataset,
    "pc12_val": bart_base_tokenized_val_aslg_pc12_dataset,
    "pc12_test": bart_base_tokenized_test_aslg_pc12_dataset,
    "ec23_train": bart_base_tokenized_train_aslg_ec23_dataset,
    "ec23_val": bart_base_tokenized_val_aslg_ec23_dataset,
    "ec23_test": bart_base_tokenized_test_aslg_ec23_dataset,
}

bart_base_loaders = build_all_dataloaders(bart_base_tokenized_datasets, bart_base_tokenizer, bart_base_model, BART_BASE_BATCH_SIZE)
bart_base_pc_12_optimizer, bart_base_pc_12_scheduler = prepare_optimizer_scheduler(bart_base_model, BART_BASE_ASLG_PC12_NUM_EPOCHS, BART_BASE_ASLG_PC12_LEARNING_RATE, BART_BASE_ASLG_PC12_WEIGHT_DECAY)
bart_base_aslg_ec23_optimizer, bart_base_aslg_ec23_scheduler = prepare_optimizer_scheduler(bart_base_model, BART_BASE_ASLG_EC23_NUM_EPOCHS, BART_BASE_ASLG_EC23_LEARNING_RATE, BART_BASE_ASLG_EC23_WEIGHT_DECAY)

"""## BART LARGE SETUP"""

BART_LARGE_BATCH_SIZE = 32
BART_LARGE_ASLG_PC12_NUM_EPOCHS = 6
BART_LARGE_ASLG_PC12_LEARNING_RATE = 1e-4
BART_LARGE_ASLG_PC12_WEIGHT_DECAY = 1e-3

BART_LARGE_ASLG_EC23_NUM_EPOCHS = 35
BART_LARGE_ASLG_EC23_LEARNING_RATE = 3e-4
BART_LARGE_ASLG_EC23_WEIGHT_DECAY = 1e-3
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

bart_large_checkpoint = "facebook/bart-large"
bart_large_model, bart_large_tokenizer = load_model_and_tokenizer(bart_large_checkpoint)

bart_large_tokenized_datasets = {
    "pc12_train": bart_large_tokenized_train_aslg_pc12_dataset,
    "pc12_val": bart_large_tokenized_val_aslg_pc12_dataset,
    "pc12_test": bart_large_tokenized_test_aslg_pc12_dataset,
    "ec23_train": bart_large_tokenized_train_aslg_ec23_dataset,
    "ec23_val": bart_large_tokenized_val_aslg_ec23_dataset,
    "ec23_test": bart_large_tokenized_test_aslg_ec23_dataset,
}

bart_large_loaders = build_all_dataloaders(bart_large_tokenized_datasets, bart_large_tokenizer, bart_large_model, BART_LARGE_BATCH_SIZE)
bart_large_aslg_pc12_optimizer, bart_large_aslg_pc12_scheduler = prepare_optimizer_scheduler(bart_large_model, BART_LARGE_ASLG_PC12_NUM_EPOCHS, BART_LARGE_ASLG_PC12_LEARNING_RATE, BART_LARGE_ASLG_PC12_WEIGHT_DECAY)
bart_large_aslg_ec23_optimizer, bart_large_aslg_ec23_scheduler = prepare_optimizer_scheduler(bart_large_model, BART_LARGE_ASLG_EC23_NUM_EPOCHS, BART_LARGE_ASLG_EC23_LEARNING_RATE, BART_LARGE_ASLG_EC23_WEIGHT_DECAY)

"""## T5 SMALL ASLG PC12"""

train_english_to_gloss_model(
    model=t5_small_model,
    tokenizer=t5_small_tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer=t5_small_tokenizer, model=t5_small_model, return_tensors="pt"),
    train_loader=t5_small_loaders["pc12_train"],
    val_loader=t5_small_loaders["pc12_val"],
    num_epochs=T5_SMALL_ASLG_PC12_NUM_EPOCHS,
    optimizer=t5_small_aslg_pc12_optimizer,
    scheduler=t5_small_aslg_pc12_scheduler,
    checkpoint_dir="./checkpoints/t5_small_pc12",
    run_name="t5_small_pc12_4/27",
    precision="bf16"
)

"""## T5 SMALL ASLG EC23"""

train_english_to_gloss_model(
    model=t5_small_model,
    tokenizer=t5_small_tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer=t5_small_tokenizer, model=t5_small_model, return_tensors="pt"),
    train_loader=t5_small_loaders["ec23_train"],
    val_loader=t5_small_loaders["ec23_val"],
    num_epochs=T5_SMALL_ASLG_EC23_NUM_EPOCHS,
    optimizer=t5_small_aslg_ec23_optimizer,
    scheduler=t5_small_aslg_ec23_scheduler,
    checkpoint_dir="./checkpoints/t5_small_ec23",
    run_name="t5_small_ec23_4/27",
    precision="bf16"
)

"""## T5 BASE ASLG PC12"""

train_english_to_gloss_model(
    model=t5_base_model,
    tokenizer=t5_base_tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer=t5_base_tokenizer, model=t5_base_model, return_tensors="pt"),
    train_loader=t5_base_loaders["pc12_train"],
    val_loader=t5_base_loaders["pc12_val"],
    num_epochs=T5_BASE_ASLG_PC12_NUM_EPOCHS,
    optimizer=t5_base_aslg_pc12_optimizer,
    scheduler=t5_base_aslg_pc12_scheduler,
    checkpoint_dir="./checkpoints/t5_base_pc12",
    run_name="t5_base_pc12_4/27",
    precision="bf16"
)

"""## T5 BASE ASLG EC23"""

train_english_to_gloss_model(
    model=t5_base_model,
    tokenizer=t5_base_tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer=t5_base_tokenizer, model=t5_base_model, return_tensors="pt"),
    train_loader=t5_base_loaders["ec23_train"],
    val_loader=t5_base_loaders["ec23_val"],
    num_epochs=T5_BASE_ASLG_EC23_NUM_EPOCHS,
    optimizer=t5_base_aslg_ec23_optimizer,
    scheduler=t5_base_aslg_ec23_scheduler,
    checkpoint_dir="./checkpoints/t5_base_ec23",
    run_name="t5_base_ec23_4/27",
    precision="bf16"
)

"""## BART BASE ASLG PC12"""

train_english_to_gloss_model(
    model=bart_base_model,
    tokenizer=bart_base_tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer=bart_base_tokenizer, model=bart_base_model, return_tensors="pt"),
    train_loader=bart_base_loaders["pc12_train"],
    val_loader=bart_base_loaders["pc12_val"],
    num_epochs=BART_BASE_ASLG_PC12_NUM_EPOCHS,
    optimizer=bart_base_pc_12_optimizer,
    scheduler=bart_base_pc_12_scheduler,
    checkpoint_dir="./checkpoints/bart_base_pc12",
    run_name="bart_base_pc12_4/27",
    precision="bf16"
)

"""## BART BASE ASLG EC23"""

train_english_to_gloss_model(
    model=bart_base_model,
    tokenizer=bart_base_tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer=bart_base_tokenizer, model=bart_base_model, return_tensors="pt"),
    train_loader=bart_base_loaders["ec23_train"],
    val_loader=bart_base_loaders["ec23_val"],
    num_epochs=BART_BASE_ASLG_EC23_NUM_EPOCHS,
    optimizer=bart_base_aslg_ec23_optimizer,
    scheduler=bart_base_aslg_ec23_scheduler,
    checkpoint_dir="./checkpoints/bart_base_ec23",
    run_name="bart_base_ec23_4/27",
    precision="bf16"
)

"""BART LARGE ASLG PC12"""

# train_english_to_gloss_model(
#     model=bart_large_model,
#     tokenizer=bart_large_tokenizer,
#     data_collator=DataCollatorForSeq2Seq(tokenizer=bart_large_tokenizer, model=bart_large_model, return_tensors="pt"),
#     train_loader=bart_large_loaders["pc12_train"],
#     val_loader=bart_large_loaders["pc12_val"],
#     num_epochs=BART_LARGE_ASLG_PC12_NUM_EPOCHS,
#     optimizer=bart_large_aslg_pc12_optimizer,
#     scheduler=bart_large_aslg_pc12_scheduler,
#     checkpoint_dir="./checkpoints/bart_large_pc12",
#     run_name="bart_large_pc12_4-18",
#     precision="bf16"
# )

"""## BART LARGE ASLG EC23"""

# train_english_to_gloss_model(
#     model=bart_large_model,
#     tokenizer=bart_large_tokenizer,
#     data_collator=DataCollatorForSeq2Seq(tokenizer=bart_large_tokenizer, model=bart_large_model, return_tensors="pt"),
#     train_loader=bart_large_loaders["ec23_train"],
#     val_loader=bart_large_loaders["ec23_val"],
#     num_epochs=BART_LARGE_ASLG_EC23_NUM_EPOCHS,
#     optimizer=bart_large_aslg_ec23_optimizer,
#     scheduler=bart_large_aslg_ec23_scheduler,
#     checkpoint_dir="./checkpoints/bart_large_ec23",
#     run_name="bart_large_ec23_4-18",
#     precision="bf16"

# )

# trained_t5_model_aslg_pc12 = train_english_to_gloss_model(model = t5_model,
#                                               tokenizer = tokenizer,
#                                               data_collator = data_collator,
#                                               train_loader = train_aslg_pc12_loader,
#                                               val_loader = val_aslg_pc12_loader,
#                                               num_epochs = num_epochs,
#                                               optimizer = optimizer,
#                                               scheduler = scheduler,
#                                               checkpoint_dir = checkpoint_dir,
#                                               run_name = "T5-Small-ASLG-PC12-Only-3/30",
#                                               cost_function = cost_function)

"""## Saving all models to hugging face"""

from huggingface_hub import notebook_login
notebook_login()

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

checkpoint_dirs = [
    "./checkpoints/t5_small_pc12/t5_small_pc12_4/27",  # Updated path for t5_small_pc12
    "./checkpoints/t5_small_ec23/t5_small_ec23_4/27",  # Updated path for t5_small_ec23
    "./checkpoints/t5_base_pc12/t5_base_pc12_4/27",  # Updated path for t5_base_pc12
    "./checkpoints/t5_base_ec23/t5_base_ec23_4/27",  # Updated path for t5_base_ec23
    "./checkpoints/bart_base_pc12/bart_base_pc12_4/27",  # Updated path for bart_base_pc12
    "./checkpoints/bart_base_ec23/bart_base_ec23_4/27",  # Updated path for bart_base_ec23

]


suffix = "_4-27"
username = "AchrafAzzaouiRiceU"

for checkpoint_dir in checkpoint_dirs:
    model_name = checkpoint_dir.split("/")[2]
    repo_name = f"{username}/{model_name}{suffix}"

    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_dir)
    tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)

    model.push_to_hub(repo_name)
    tokenizer.push_to_hub(repo_name)

    print(f"Uploaded {repo_name}!")

"""# Evaluating all models on test set"""

import torch
import numpy as np
import wandb
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


model_names = [
    "AchrafAzzaouiRiceU/t5_small_pc12_4-27",
    "AchrafAzzaouiRiceU/t5_small_ec23_4-27",
    "AchrafAzzaouiRiceU/t5_base_pc12_4-27",
    "AchrafAzzaouiRiceU/t5_base_ec23_4-27",
    "AchrafAzzaouiRiceU/bart_base_pc12_4-27",
    "AchrafAzzaouiRiceU/bart_base_ec23_4-27",
]


test_datasets = {
    "t5_small_pc12": t5_small_loaders['pc12_test'],
    "t5_small_ec23": t5_small_loaders['ec23_test'],
    "t5_base_pc12": t5_base_loaders['pc12_test'],
    "t5_base_ec23": t5_base_loaders['ec23_test'],
    "bart_base_pc12": bart_base_loaders['pc12_test'],
    "bart_base_ec23": bart_base_loaders['ec23_test'],
}

def test_english_to_gloss_model_with_new_graph(model, tokenizer, test_dataset, compute_metrics, run_name=None):
    model.eval()
    test_preds = []
    test_labels = []
    cumulative_test_loss = 0.0
    num_test_batches = 0


    if run_name is None:
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        run_name = f"english_to_gloss_{timestamp}"


    # wandb.init(
    #     project="Comp646EnglishToGloss",
    #     entity="aa270-rice-university",
    #     reinit=True,
    #     name=run_name
    # )

    with torch.no_grad():
        for batch_id, batch in enumerate(test_dataset):
            print(f"Testing batch {batch_id + 1}/{len(test_dataset)}")
            xb = batch['input_ids'].to(device)
            yb = batch['labels'].to(device)
            predicted = model(input_ids=xb, labels=yb)
            loss = predicted.loss
            cumulative_test_loss += loss.item()
            num_test_batches += 1

            generated_ids = model.generate(input_ids=xb, max_length=128)
            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
            labels = np.where(yb.cpu().numpy() != -100, yb.cpu().numpy(), tokenizer.pad_token_id)
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
            test_preds.extend(decoded_preds)
            test_labels.extend(decoded_labels)

    results = compute_metrics((test_preds, test_labels), tokenizer)
    test_loss = cumulative_test_loss / num_test_batches
    print(f"Test Loss: {test_loss:.4f}")

    metrics_data = []
    for metric_name, metric_value in results.items():
        if isinstance(metric_value, (int, float)):
            metrics_data.append([metric_name, metric_value])

    wandb.log({
        "accuracy_metrics_chart": wandb.plot.bar(
            table=wandb.Table(data=metrics_data, columns=["metric", "score"]),
            label="metric",
            value="score",
            title="Model Accuracy Metrics"
        )
    })

    wandb.log({"test_loss": test_loss, **results})



    return results, test_loss


for model_name in model_names:
    print(f"🔵 Loading {model_name}...")
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model.to(device)

    short_model_name = model_name.split("/")[-1].replace("_4-27", "")
    test_loader = test_datasets[short_model_name]


    wandb.init(
        project="Comp646EnglishToGloss",
        entity="aa270-rice-university",
        name=f"{short_model_name}_test_eval_4-27",
        reinit=True
    )

    results, test_loss = test_english_to_gloss_model_with_new_graph(
        model,
        tokenizer,
        test_loader,
        compute_metrics,
        run_name=f"{short_model_name}_test_eval_4-27"
    )

    print(f"✅ Done testing {short_model_name} — Loss: {test_loss:.4f}")


    wandb.finish()

import pandas as pd

model_names = [
    "AchrafAzzaouiRiceU/t5_small_pc12_4-27",
    "AchrafAzzaouiRiceU/t5_small_ec23_4-27",
    "AchrafAzzaouiRiceU/t5_base_pc12_4-27",
    "AchrafAzzaouiRiceU/t5_base_ec23_4-27",
    "AchrafAzzaouiRiceU/bart_base_pc12_4-27",
    "AchrafAzzaouiRiceU/bart_base_ec23_4-27",
]

# Your test datasets per model (ASSUMPTION: you have them)
# Example: test_datasets = {"t5_small_pc12": test_aslg_pc12_loader, ...}
# If all use the same test set, just point all to `test_aslg_pc12_loader`

test_datasets = {
    "t5_small_pc12": t5_small_loaders['pc12_test'],
    "t5_small_ec23": t5_small_loaders['ec23_test'],
    "t5_base_pc12": t5_base_loaders['pc12_test'],
    "t5_base_ec23": t5_base_loaders['ec23_test'],
    "bart_base_pc12": bart_base_loaders['pc12_test'],
    "bart_base_ec23": bart_base_loaders['ec23_test'],
}


def generate_full_english_gloss_triples(model, tokenizer, test_dataset):
    model.eval()
    english_sentences = []
    model_outputs = []
    ground_truth_glosses = []

    with torch.no_grad():
        for batch in test_dataset:
            xb = batch['input_ids'].to(device)
            yb = batch['labels'].to(device)

            generated_ids = model.generate(input_ids=xb, max_length=128)
            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
            decoded_inputs = tokenizer.batch_decode(xb, skip_special_tokens=True)
            labels = np.where(yb.cpu().numpy() != -100, yb.cpu().numpy(), tokenizer.pad_token_id)
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            english_sentences.extend(decoded_inputs)
            model_outputs.extend(decoded_preds)
            ground_truth_glosses.extend(decoded_labels)


    samples = []
    for idx in range(len(english_sentences)):
        samples.append({
            "English Input": english_sentences[idx],
            "Model Output Gloss": model_outputs[idx],
            "Ground Truth Gloss": ground_truth_glosses[idx],
        })

    return samples

for model_name in model_names:

    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model.to(device)

    short_model_name = model_name.split("/")[-1].replace("_4-27", "")
    test_loader = test_datasets[short_model_name]

    full_samples = generate_full_english_gloss_triples(
        model,
        tokenizer,
        test_loader
    )

    samples_df = pd.DataFrame(full_samples)


    save_filename = f"{short_model_name}_full_test_outputs.csv"
    samples_df.to_csv(save_filename, index=False)
    print(f"✅ Saved full test set outputs to {save_filename}")

# (Optional) Create a folder if you want to stay organized
!mkdir -p saved_csvs
!mv *.csv saved_csvs/

# Zip the folder
!zip -r saved_csvs.zip saved_csvs/
from google.colab import files
files.download("saved_csvs.zip")

checkpoint_dir = "t5_model_english_to_gloss_aslg_ec23_v1"
trained_t5_model_aslg_ec23 = train_english_to_gloss_model(model = t5_model,
                                              tokenizer = tokenizer,
                                              data_collator = data_collator,
                                              train_loader = train_aslg_ec23_loader,
                                              val_loader = val_aslg_ec23_loader,
                                              num_epochs = 30,
                                              optimizer = optimizer,
                                              scheduler = scheduler,
                                              checkpoint_dir = checkpoint_dir,
                                              run_name = "T5-Small-ASLG-EC23-Only-3/30",
                                              cost_function = cost_function)

import os
print(f"Current working directory: {os.getcwd()}")
print(f"Model will be saved to: {os.path.abspath('t5_model_english_to_gloss')}")

from huggingface_hub import notebook_login
notebook_login()

trained_t5_model_aslg_ec23.push_to_hub("AchrafAzzaouiRiceU/t5-english-to-asl-gloss-aslg-ec23-only-v1")
tokenizer.push_to_hub("AchrafAzzaouiRiceU/t5-english-to-asl-gloss-aslg-ec23-only-v1")

from huggingface_hub import notebook_login
notebook_login()

trained_model.push_to_hub("AchrafAzzaouiRiceU/t5-english-to-asl-gloss")
tokenizer.push_to_hub("AchrafAzzaouiRiceU/t5-english-to-asl-gloss")

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

model_name = "AchrafAzzaouiRiceU/t5-english-to-asl-gloss-aslg-pc12-only"
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prefix = "translate English to Gloss: "
english_text = "I ate a large apple today"
input_text = prefix + english_text

input_ids = tokenizer(input_text, return_tensors="pt").input_ids
outputs = model.generate(input_ids=input_ids, max_length=128)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(decoded)

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
aslg_pc12_only_model_name = "AchrafAzzaouiRiceU/t5-english-to-asl-gloss-aslg-pc12-only-v1"
aslg_pc12_only_model = AutoModelForSeq2SeqLM.from_pretrained(aslg_pc12_only_model_name)
aslg_pc12_only_tokenizer = AutoTokenizer.from_pretrained(aslg_pc12_only_model_name)

aslg_pc12_only_model.to(device)

def test_english_to_gloss_model_with_new_graph(model, tokenizer, test_dataset, compute_metrics, run_name=None):
    model.eval()
    test_preds = []
    test_labels = []
    cumulative_test_loss = 0.0
    num_test_batches = 0

    if not wandb.run:
        if run_name is None:
            import datetime
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            run_name = f"english_to_gloss_{timestamp}"

        wandb.init(
            project="Comp646EnglishToGloss",
            entity="aa270-rice-university",
            reinit = True,
            name=run_name
        )

    with torch.no_grad():
        for batch_id, batch in enumerate(test_dataset):
            print(f"Testing batch {batch_id + 1}/{len(test_dataset)}")
            xb = batch['input_ids'].to(device)
            yb = batch['labels'].to(device)
            predicted = model(input_ids=xb, labels=yb)
            loss = predicted.loss
            cumulative_test_loss += loss.item()
            num_test_batches += 1

            generated_ids = model.generate(input_ids=xb, max_length=128)
            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
            labels = np.where(yb.cpu().numpy() != -100, yb.cpu().numpy(), tokenizer.pad_token_id)
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
            test_preds.extend(decoded_preds)
            test_labels.extend(decoded_labels)

    results = compute_metrics((test_preds, test_labels), tokenizer)
    test_loss = cumulative_test_loss / num_test_batches
    print(f"Test Loss: {test_loss:.4f}")

    metrics_data = []
    for metric_name, metric_value in results.items():
        if isinstance(metric_value, (int, float)):  # Ensure it's a number
            metrics_data.append([metric_name, metric_value])

    # Create and log the bar chart
    wandb.log({
        "accuracy_metrics_chart": wandb.plot.bar(
            table=wandb.Table(data=metrics_data, columns=["metric", "score"]),
            label="metric",
            value="score",
            title="Model Accuracy Metrics"
        )
    })

    # Also log the individual metrics as before
    wandb.log({"test_loss": test_loss, **results})

    return results, test_loss


aslg_pc12_only_model_results, test_loss = test_english_to_gloss_model_with_new_graph(
    aslg_pc12_only_model,
    aslg_pc12_only_tokenizer,
    test_aslg_pc12_loader,
    compute_metrics,
    run_name="T5-Small-ASLG-PC12-Only-3/30-TestSetEval"
)

# Keep the run active
# wandb.finish()  # Only call this when you're completely done

wandb.finish()
aslg_ec23_only_model_name = "AchrafAzzaouiRiceU/t5-english-to-asl-gloss-aslg-ec23-only-v1"
aslg_ec23_only_model = AutoModelForSeq2SeqLM.from_pretrained(aslg_ec23_only_model_name)
alsg_ec23_only_tokenizer = AutoTokenizer.from_pretrained(aslg_ec23_only_model_name)

aslg_ec23_only_model.to(device)

aslg_ec23_only_model_results, test_loss = test_english_to_gloss_model_with_new_graph(
    aslg_ec23_only_model,
    alsg_ec23_only_tokenizer,
    test_aslg_ec23_loader,
    compute_metrics,
    run_name="T5-Small-ASLG-EC23-Only-3/30-TestSetEval"
)